# Compara√ß√£o de desempenho ‚Äî Pandas vs PySpark Pandas

O objetivo deste experimento √© comparar o **tempo de leitura** de um arquivo `clientes.json` contendo **10 milh√µes de linhas** (aproximadamente **661 MB**) utilizando as bibliotecas **Pandas** e **PySpark Pandas** (*pandas-on-Spark*).

---

## ‚öôÔ∏è Ambiente de execu√ß√£o

Os testes foram realizados em **m√°quina local**, com as seguintes especifica√ß√µes:

- **Processador:** Intel Core i5 (12 threads)  
- **Mem√≥ria RAM:** 16 GB  
- **Sistema:** Linux  
- **Arquivo de entrada:** `clientes.json` (~661 MB, 10 milh√µes de registros, formato JSON com `lines=True`)

---

## üß™ Metodologia

O tempo total foi medido utilizando a fun√ß√£o `time.time()`, considerando desde o in√≠cio da leitura at√© a cria√ß√£o completa do DataFrame.

### Leitura com Pandas

```python
import pandas as pd
import time

start = time.time()
df_pandas = pd.read_json("clientes.json", lines=True)
print(f"Tempo (Pandas): {time.time() - start:.2f} segundos")
```

### Leitura com PySpark Pandas

```python
import pyspark.pandas as ps
import time
from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("Comparacao_Pandas_PySparkPandas")
    .config("spark.sql.execution.arrow.pyspark.enabled", "true")
    .config("spark.sql.ansi.enabled", "false")
    .getOrCreate()
)

start = time.time()
df_ps = ps.read_json("clientes.json", lines=True)
print(f"Tempo (PySpark Pandas): {time.time() - start:.2f} segundos")
```

---

## üìä Resultados obtidos

| Biblioteca         | Tempo de leitura | Diferen√ßa relativa | Observa√ß√µes |
|--------------------|------------------|--------------------|--------------|
| **Pandas**         | 18,71 s | ‚Äî | Processamento sequencial em mem√≥ria |
| **PySpark Pandas** | **1,81 s** | **~10,3√ó mais r√°pido** | Aproveitou o paralelismo das 12 threads via Spark |

---

## üß† An√°lise

Mesmo com um arquivo de apenas 661 MB, o **PySpark Pandas** apresentou desempenho muito superior.  
Isso ocorre porque o **Spark** divide automaticamente o trabalho em v√°rias parti√ß√µes e executa a leitura de forma **paralela**, enquanto o **Pandas** utiliza apenas um n√∫cleo da CPU.  

Al√©m disso, a ativa√ß√£o do **Apache Arrow** (`spark.sql.execution.arrow.pyspark.enabled=true`) reduziu a sobrecarga de convers√£o entre Spark e Pandas, otimizando ainda mais a leitura.

---

## üß© Conclus√£o

- O **Pandas** continua sendo uma op√ß√£o simples e eficiente para arquivos pequenos e m√©dios.  
- Entretanto, mesmo em um **ambiente local**, o **PySpark Pandas** demonstrou ganhos expressivos de desempenho gra√ßas ao **processamento paralelo**.  
- Esse resultado evidencia que o **Spark** pode ser vantajoso n√£o apenas em clusters, mas tamb√©m em **m√°quinas multicore** com hardware moderno.
